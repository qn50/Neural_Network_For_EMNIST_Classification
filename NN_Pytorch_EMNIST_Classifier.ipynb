{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Train an EMNIST classifier\n",
        "\n",
        "EMNIST (or extended MNIST) is just like MNIST, a bunch of handwritten images, except instead of just digits (0-9) it also has uppercase and lowercase characters.\n",
        "\n",
        "You are provided with the code to load the EMNIST train and split **datasets**\n",
        "\n",
        "Write and train a classifier for EMNIST. Make sure to\n",
        "\n",
        "- Show how your loss(es) dropped during training.\n",
        "- Use the testset as a validation set duirng training.\n",
        "- Show final performance of your model on untrained data.\n",
        "- Maintain good code quality.\n"
      ],
      "metadata": {
        "id": "zemWtDr8gVOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "4GQMlmw2hwbn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch torchvision\n",
        "%pip install matplotlib\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "cvcXCUETh0AW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8j3LQ1Qza-vV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.datasets import EMNIST\n",
        "import torchvision.transforms.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = EMNIST(root='emnist_data/', split='byclass', download=True, transform=to_tensor)\n",
        "test_data = EMNIST(root='emnist_data/', split='byclass', download=True, train=False, transform=to_tensor)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "InCfHKZme7ga"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for demo purposes. feel free to remove it if you want.\n",
        "print('Number of classes in dataset:', len(train_data.classes))\n",
        "print('Unique labels:', train_data.classes)\n",
        "\n",
        "demo_img, demo_label = train_data[10]\n",
        "\n",
        "print(demo_label)\n",
        "F.resize(demo_img, (256, 256)) #resizing just for display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udb4rY5Vhaau",
        "outputId": "8bff6700-c036-4a62-99bf-d5ede6a5602e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes in dataset: 62\n",
            "Unique labels: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Impelementing the model\n"
      ],
      "metadata": {
        "id": "59qSiLaS4Itg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Dataloaders are used to easily create batches of data so we can perform batch gradient descent for faster learning\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "yqzFSnzGF6aH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN1Layer(nn.Module):\n",
        "\n",
        "  def __init__(self, num_inp, num_out):\n",
        "\n",
        "    super(NN1Layer, self).__init__()\n",
        "\n",
        "    self.layer_1 = nn.Linear(num_inp, num_out)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    z = self.layer_1(x)\n",
        "    a = self.softmax(z)\n",
        "\n",
        "    return a\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NN2Layer(nn.Module):\n",
        "\n",
        "  def __init__(self, num_inp, num_hidden, num_out):\n",
        "\n",
        "    super(NN2Layer, self).__init__()\n",
        "\n",
        "    self.layer_1 = nn.Linear(num_inp, num_hidden)\n",
        "    self.layer_2 = nn.Linear(num_hidden, num_out)\n",
        "\n",
        "    self.hidden_activation = nn.ReLU()  # We can change the hidden activation (activation in between layer 1 and 2) here\n",
        "    self.softmax = nn.Softmax(dim=1)  # dim 0 is normally batch size, we don't want to apply softmax across batch size\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    z1 = self.layer_1(x)\n",
        "    a1 = self.hidden_activation(z1)\n",
        "\n",
        "    z2 = self.layer_2(a1)\n",
        "    a2 = self.softmax(z2)\n",
        "\n",
        "    return a2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EBTno786WzyA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 12\n",
        "lr = 1e-4\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # checks if machine supports cuda and if it does, we use that, otherwise cpu\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "model = NN2Layer(28*28, 130,62)  # The 2 layer one is equivalent to the one we implemented in numpy\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()  # multi-class\n",
        "\n",
        "model.to(device)  # we need to send all input tensors as well as our model to this device. by default they are on cpu\n",
        "\n",
        "print(f'Using device {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyQ6RfvsW1nN",
        "outputId": "9c7df93c-d175-44b3-b858-78d867c2ecfa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch_no in range(num_epochs):\n",
        "\n",
        "  model.train()  # convert to train model. This turns out train-specific layers in the model (if you dont know about them, an example of them is dropout. more on this later)\n",
        "\n",
        "  epoch_weighted_loss = 0\n",
        "\n",
        "  for batch_X, batch_y in train_loader:\n",
        "\n",
        "    batch_X = batch_X.view(-1, 28*28).to(device)  # convert to [N, 28*28] shape where N is batch_size\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    batch_y_probs = model(batch_X)  # outputs [N, 10] where each [:, 10] is probabilities for class (0-9)\n",
        "\n",
        "    loss = criterion(batch_y_probs, batch_y)\n",
        "\n",
        "    optimizer.zero_grad()  # need to clear out gradients from previous batch\n",
        "    loss.backward()  # calculate new gradients\n",
        "    optimizer.step()  # update weights\n",
        "\n",
        "    epoch_weighted_loss += (len(batch_y)*loss.item())\n",
        "\n",
        "  epoch_loss = epoch_weighted_loss/len(train_loader.dataset)\n",
        "  train_losses.append(epoch_loss)    # add loss for tracking. we'll visualize the loss trajectory later\n",
        "\n",
        "\n",
        "  # validation time\n",
        "\n",
        "  model.eval()  # take model to evaluation mode. turn off train-only layers\n",
        "  correctly_labelled = 0\n",
        "\n",
        "  with torch.no_grad():  # this makes our model to NOT track gradients\n",
        "\n",
        "    val_epoch_weighted_loss = 0\n",
        "\n",
        "    for val_batch_X, val_batch_y in test_loader:\n",
        "\n",
        "      val_batch_X = val_batch_X.view(-1, 28*28).to(device)\n",
        "      val_batch_y = val_batch_y.to(device)\n",
        "\n",
        "      val_batch_y_probs = model(val_batch_X)\n",
        "\n",
        "      loss = criterion(val_batch_y_probs, val_batch_y)\n",
        "      val_epoch_weighted_loss += (len(val_batch_y)*loss.item())\n",
        "\n",
        "      val_batch_y_pred = val_batch_y_probs.argmax(dim=1)  # convert probailities to labels by picking the label (index) with the highest prob\n",
        "\n",
        "      correctly_labelled += (val_batch_y_pred == val_batch_y).sum().item()  # item converts tensor to float/int/list\n",
        "\n",
        "  val_epoch_loss = val_epoch_weighted_loss/len(test_loader.dataset)\n",
        "  val_losses.append(val_epoch_loss)\n",
        "\n",
        "  print(f'Epoch: {epoch_no}, train_loss={epoch_loss}, val_loss={val_epoch_loss}. labelled {correctly_labelled}/{len(test_loader.dataset)} correctly ({correctly_labelled/len(test_loader.dataset)*100}% accuracy)')\n",
        "\n",
        "print(f'Training complete on device {device}. Change device variable and run again to see the difference.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7wn0uamXAN_",
        "outputId": "fa9c01e0-3b90-43bc-c556-8069b184c73b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, train_loss=3.727468067796118, val_loss=3.6730114670000176. labelled 56652/116323 correctly (48.70232026340449% accuracy)\n",
            "Training complete on device cuda. Change device variable and run again to see the difference.\n",
            "CPU times: user 1min 43s, sys: 890 ms, total: 1min 44s\n",
            "Wall time: 1min 45s\n"
          ]
        }
      ]
    }
  ]
}